#!/bin/bash

# Pre-requisites
#
# Spark without Hadoop
#   wget https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-without-hadoop.tgz
#
# Hadoop
#   wget https://archive.apache.org/dist/hadoop/core/hadoop-3.3.6/hadoop-3.3.6.tar.gz
#
# Create a $SPARK_HOME/conf/spark-env.sh in file with the following
#   export SPARK_DIST_CLASSPATH=$(~/dev/tools/dist/hadoop-3.3.6/bin/hadoop classpath)

export SPARK_HOME=~/dev/tools/dist/spark-3.5.1-bin-without-hadoop/

PWD=$(dirname $0)

# Note that the AWS version must match the version that Hadoop built against
# See http://hadoop.apache.org/docs/r3.1.0/hadoop-aws/tools/hadoop-aws/troubleshooting_s3a.html#Classpath_Setup
# And https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws/3.3.6
HADOOP_VERSION=3.3.6
AWS_VERSION=1.12.751

PACKAGES=
PACKAGES+=com.amazonaws:aws-java-sdk:$AWS_VERSION
PACKAGES+=,org.apache.hadoop:hadoop-aws:$HADOOP_VERSION
PACKAGES+=,org.apache.hadoop:hadoop-common:$HADOOP_VERSION

REPOSITORIES=https://repo.typesafe.com/typesafe/ivy-releases/,https://repo1.maven.org/maven2/

OPTIONS=
OPTIONS+=\ --master\ local[*]
OPTIONS+=\ --conf\ spark.driver.maxResultSize=0
OPTIONS+=\ --conf\ spark.ui.port=5050
OPTIONS+=\ --conf\ spark.sql.parquet.writeLegacyFormat=true
OPTIONS+=\ --driver-memory\ 5G\ --executor-memory\ 5G
SPARKSHELL=$SPARK_HOME/bin/spark-shell

# echo "WARNING: Some weirdness with the latest version is causing intermittent S3 SSL issues"
# echo "         Check S3 connectivity immediately before running by S3 copying a file locally from S3"

export SPARK_LOCAL_IP=127.0.0.1
$SPARKSHELL --packages $PACKAGES --repositories $REPOSITORIES $OPTIONS -i $PWD/run-spark-shell.scala
